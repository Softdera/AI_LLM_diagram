Large Language Models (LLMs) do not understand text directly. Instead, they first
break text into small pieces called tokens. Tokenization converts words, parts
of words, or symbols into numerical values that the model can process. This step
allows the model to work with text in a structured and consistent way.

After tokenization, the tokens pass through several transformer blocks. Each
transformer block uses an attention mechanism, which helps the model focus on
important tokens and understand context within a sentence. By stacking many
transformer blocks, the model learns relationships between words and can produce
useful outputs such as predictions or generated text.
